{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Intro to Big Data\n",
    "\n",
    "Big data is the term we use when the data exceeds the processing capacity of typical databases and machines. We need to use alternate methods of big data analytics when the data grows quickly and we need to uncover hidden patterns, unknown correlations, and other useful information that machine learning often provides. There are three main features in big data (the 3 \"V\"s if you will):\n",
    "- **Volume**: Large amounts of data\n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data\n",
    "- **Velocity**: Needs to be analyzed quickly\n",
    "\n",
    "Sinan's 4th V (unofficial big data tenet):\n",
    "- **Value**: It's important to assess the value of our data and models and how they provide business/practical value.  Understanding the costs vs benefits of different approaches to problems becomes even more essential in the context of big data.  It's easy to misundersatnd the 3 V's without looking at the bigger picture, connecting the value of the business cases involved.\n",
    "\n",
    "![3v](images/3vbigdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and Conquer\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "Divide and conquer strategy is a fundamental algorithmic technique for solving a given task, whose steps include:\n",
    "\n",
    "1. split task into subtasks\n",
    "- solve these subtasks independently\n",
    "- recombine the subtask results into a final result\n",
    "\n",
    "The defining characteristic of a problem that is suitable for the divide and conquer approach is that it can be broken down into independent subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "<img src=\"images/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mapper for a word count\n",
    "from operator import itemgetter\n",
    "\n",
    "def map_reduce_count_words(texts):\n",
    "    counter = []                                                             # keep track of the reduction\n",
    "    word_single_counts = []                                                  # keep track of the mapping\n",
    "    for line in texts:                                                       # for each sentence\n",
    "        line = line.strip()\n",
    "        words = line.split()                                                 # tokenize sentences by words\n",
    "        for word in words:\n",
    "            word_single_counts.append((word, 1))                             # create (word, 1) tuples for each word\n",
    "        word_single_counts = sorted(word_single_counts, key=lambda x: x[0])  # add to our mapping list\n",
    "\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    word = None\n",
    "\n",
    "    for word, count in word_single_counts:                                   # this step would be parallelized\n",
    "        if current_word == word:                                             # we are adding up the single counts\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word:\n",
    "                counter.append((current_word, current_count))\n",
    "            current_count = count\n",
    "            current_word = word\n",
    "\n",
    "    # do not forget to output the last word if needed!\n",
    "    if current_word == word:\n",
    "        counter.append((current_word, current_count))\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 2), ('hello', 3), ('hi', 3), ('how', 1), ('you', 2)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Word Count\n",
    "\n",
    "map_reduce_count_words(['hello', \"hi hello\", \"hi how are you\", \"hello you are hi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's grab a decent volume at a decent velocity of job descriptions in NYC\n",
    "# Let's then look at the vocabulary used by companies to see what they are really looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# grab postings from the web\n",
    "texts = []\n",
    "for i in range(0,1000,10): # cycle through 100 pages of indeed job resources\n",
    "    print i, # Show the index of the page we are on\n",
    "    # Get the html of the job listings\n",
    "    soup = BeautifulSoup(requests.get('http://www.indeed.com/jobs?l=New York, NY&q=data+scientist&start='+str(i)).text, 'html.parser')\n",
    "    # clean them up, add them to a list\n",
    "    texts += [a.text for a in soup.findAll('span', {'class':'summary'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 job descriptions\n"
     ]
    }
   ],
   "source": [
    "print len(texts), \"job descriptions\" # 1,500 descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Senior Data Scientist. Integrating with external data sources and APIs to discover interesting trends (NOAA Weather Data + Credit Card Transactions =...'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]   # the first job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = map_reduce_count_words(texts)\n",
    "\n",
    "# Use our map reduce function to get the top 10 used words in this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'for', 399),\n",
       " (u'with', 429),\n",
       " (u'Data', 442),\n",
       " (u'a', 505),\n",
       " (u'the', 546),\n",
       " (u'of', 563),\n",
       " (u'in', 601),\n",
       " (u'to', 860),\n",
       " (u'data', 1031),\n",
       " (u'and', 1583)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_counts, key=lambda x:x[1])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This doesn't seem as robust as we might need\n",
    "\n",
    "# I see all of these useless words like is, in with, the, of\n",
    "# Also How do I get 2 word phrases like \"machine learning\"\n",
    "\n",
    "# This data is failing Sinan's fourth V, Value! How would I ever use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x10836 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 41520 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################\n",
    "##### Machine Learning ######\n",
    "#############################\n",
    "\n",
    "# A built in counter of words built in scikit-learn (a python maching learning module)\n",
    "# The best thing about big data is that if we can create a process for a smaller set, \n",
    "# It will also work for larger sets\n",
    "# So let's create a process for this smaller set\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "# - ngram_range: look for one and two word phrases in our job descriptions\n",
    "# - stop_words: ignore words like \"I\", \"you\" \"am\", \"and\", \"or\"\n",
    "# - min_df: When building the vocabulary ignore terms that have a document frequency \n",
    "# strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "\n",
    "matrix = vect.fit_transform(texts)\n",
    "# fit and learn to the vocabulary in the corpus\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are about 10,000 single and double word phrases in 1,500 job descriptions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 1570\n",
      "learning 443\n",
      "machine 336\n",
      "machine learning 334\n",
      "scientist 308\n",
      "scientists 298\n",
      "data scientist 297\n",
      "analytics 294\n",
      "new 273\n",
      "sql 262\n",
      "experience 256\n",
      "trends 238\n",
      "business 186\n",
      "insights 171\n",
      "data scientists 165\n",
      "quantitative 162\n",
      "team 154\n",
      "deep 152\n",
      "looking 151\n",
      "research 151\n",
      "company 149\n",
      "techniques 143\n",
      "senior 142\n",
      "models 138\n",
      "python 137\n"
     ]
    }
   ],
   "source": [
    "# top 25 word phrases\n",
    "\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "for phrase, times in sorted (freqs, key = lambda x: -x[1])[:25]:\n",
    "    print phrase, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning 334\n",
      "data scientist 297\n",
      "data scientists 165\n",
      "new york 123\n",
      "senior data 121\n",
      "data sources 112\n",
      "scientist analytics 108\n",
      "york city 107\n",
      "motivated data 107\n",
      "data assets 107\n",
      "company looking 106\n",
      "looking talented 106\n",
      "deep learning 105\n",
      "talented motivated 105\n",
      "provide business 104\n",
      "developer help 104\n",
      "insight data 104\n",
      "analytics developer 104\n",
      "business insight 104\n",
      "help provide 104\n",
      "external data 103\n",
      "experience sql 103\n",
      "quantitative techniques 103\n",
      "state art 103\n",
      "insurance company 103\n"
     ]
    }
   ],
   "source": [
    "# What if we looked ONLY at 2 word phrases\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(2,2), stop_words='english')\n",
    "matrix = vect.fit_transform(texts)\n",
    "freqs = [(word, matrix.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()]\n",
    "for phrase, times in sorted (freqs, key = lambda x: -x[1])[:25]:\n",
    "    print phrase, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x37753 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 94992 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Volume, let's see how many phrases there are if we consider up to 5 word phrases\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,5), stop_words='english')\n",
    "# - ngram_range: look for one and two word phrases in our job descriptions\n",
    "# - stop_words: ignore words like \"I\", \"you\" \"am\", \"and\", \"or\"\n",
    "# - min_df: When building the vocabulary ignore terms that have a document frequency \n",
    "# strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "\n",
    "matrix = vect.fit_transform(texts)\n",
    "# fit and learn to the vocabulary in the corpus\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# over 37 thousand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n"
     ]
    }
   ],
   "source": [
    "# Let's count the number of phrases there are for a variety of numbers\n",
    "\n",
    "num_phrases = []\n",
    "for i in range(1,20):\n",
    "    print i,\n",
    "    vect = CountVectorizer(ngram_range=(1,i), stop_words='english')\n",
    "    matrix = vect.fit_transform(texts)\n",
    "    num_phrases.append(matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1101cdad0>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFhCAYAAACh/xvXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcdBRfVJkaJZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f434810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd # our data manipulation module\n",
    "df = pd.DataFrame({'phrase limit': range(1,20), 'number of phrases': num_phrases})\n",
    "import seaborn as sns # our pretty graph module\n",
    "%matplotlib inline   \n",
    "\n",
    "\n",
    "sns.lmplot(x=\"phrase limit\", y=\"number of phrases\", data=df, ci=None, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thought experiment: Why is there a dropoff?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.720666666666666"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because on average our summary lengths aren't THAT long so at around 19, \n",
    "# some descriptions aren't producing anynew phrases\n",
    "\n",
    "\n",
    "\n",
    "num_words = [len(t.split()) for t in texts]\n",
    "average_num_words = sum(num_words) / float(len(num_words))\n",
    "\n",
    "average_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "LDA - Latent Dirichlet Allocation\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "Why:   Much quicker than manually creating and identifying topic clusters\n",
    "'''\n",
    "\n",
    "import lda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: data, scientists, data scientists, work, team, science, project, engineers, analysis\n",
      "Topic 1: data, scientist, analytics, data scientist, business, looking, provide, company, help\n",
      "Topic 2: data, trends, senior data, scientist, sources, senior, data scientist, senior data scientist, data sources\n",
      "Topic 3: new, experience, sql, insights, quantitative, market, ability, york, new york\n",
      "Topic 4: learning, machine, machine learning, data, deep, applications, diverse, deep learning, state\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=5, n_iter=100)\n",
    "model.fit(matrix) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommending stores to shop at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# There are two general approaches to the design:\n",
    "\n",
    "## In content-based filtering, items are mapped into a feature space, and recommendations depend on item characteristics.\n",
    "\n",
    "<img src='images/content.png'>\n",
    "\n",
    "\n",
    "## In contrast, the only data under consideration in collaborative filtering are user-item ratings, and recommendations depend on user preferences.\n",
    "\n",
    "<img src='images/collab.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_brands = pd.read_csv('data/user_brand.csv') # user store preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80002</td>\n",
       "      <td>Target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80002</td>\n",
       "      <td>Home Depot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80010</td>\n",
       "      <td>Levi's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80010</td>\n",
       "      <td>Puma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80010</td>\n",
       "      <td>Cuisinart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       Store\n",
       "0  80002      Target\n",
       "1  80002  Home Depot\n",
       "2  80010      Levi's\n",
       "3  80010        Puma\n",
       "4  80010   Cuisinart"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_brands.head(5) # Each row is a tuple that represents a person liking a single store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note this list will have duplicates if a person likes multiple stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target             1866\n",
       "Old Navy           1200\n",
       "Home Depot         1186\n",
       "Kohl's             1157\n",
       "Banana Republic     932\n",
       "Name: Store, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_brands.Store.value_counts().head() # the top loved stores, do you agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method takes in a person's unique ID and returns the brands they like\n",
    "\n",
    "def brandsfor(person_id):\n",
    "    return list(user_brands[user_brands.ID==person_id]['Store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Levi's\",\n",
       " 'Puma',\n",
       " 'Cuisinart',\n",
       " 'Converse',\n",
       " 'DKNY',\n",
       " 'Express',\n",
       " \"Kohl's\",\n",
       " 'Old Navy',\n",
       " 'Container Store',\n",
       " 'Nordstrom']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brandsfor(80010) # Damn, slow down and stop shopping so much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/jaccard.png'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the Jaccard Similarty of sets, we can quantify how close people are as shoppers\n",
    "\n",
    "def person_similarity(first, second):\n",
    "    first = set(first)\n",
    "    second = set(second)\n",
    "    return float(len(first & second)) / len(first | second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_similarity(brandsfor(80010), brandsfor(80011))\n",
    "# The jaccard similarity is bounded by 0 and 1\n",
    "# So the closer to 0 the more DISsimilar\n",
    "# The closer to 1 the more SIMilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try to recommend a brand to this person\n",
    "\n",
    "new_user = ['Target', 'Banana Republic', 'H&M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get a list of all of the unique users\n",
    "all_user_ids = user_brands['ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of similarities of our new_user to each person in that list\n",
    "similar_users = zip(all_user_ids, map(lambda x:person_similarity(brandsfor(x), new_user), all_user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(80002, 0.25),\n",
       " (80010, 0.0),\n",
       " (80011, 0.13333333333333333),\n",
       " (80015, 0.4),\n",
       " (80020, 0.0)]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_users[:5] # Here are five people and their similarty to the new_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_similar_users = 10 # Let's look at the closest 10 people\n",
    "most_similar_users = sorted(similar_users, key=lambda x:x[1], reverse=True)[:num_similar_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(86065, 0.6666666666666666),\n",
       " (80185, 0.5),\n",
       " (81012, 0.5),\n",
       " (82716, 0.5),\n",
       " (84720, 0.5),\n",
       " (90787, 0.5),\n",
       " (80015, 0.4),\n",
       " (81802, 0.4),\n",
       " (82452, 0.4),\n",
       " (82602, 0.4)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's grab the brands that the 5 most similar people like\n",
    "most_similar_brands = [brandsfor(person_id) for person_id, similarity_score in most_similar_users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Banana Republic', 'Target'],\n",
       " ['Banana Republic', 'Target', 'Shoebuy'],\n",
       " ['Banana Republic', 'Old Navy', 'Target'],\n",
       " ['Banana Republic', 'Target', 'Nordstrom'],\n",
       " ['Banana Republic', 'Gap', 'Target'],\n",
       " ['Banana Republic', 'Gap', 'Target'],\n",
       " ['Banana Republic', 'Gap', 'Target', 'Home Depot'],\n",
       " ['Banana Republic', 'Express', \"Kohl's\", 'Target'],\n",
       " ['Banana Republic', 'Kate Spade', \"Kohl's\", 'Target'],\n",
       " ['Banana Republic', '6pm.com', 'Target', 'Nordstrom']]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put them in one \"giant\" list\n",
    "most_similar_brands = reduce(lambda x, y: x+y, most_similar_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banana Republic',\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " 'Target',\n",
       " 'Shoebuy',\n",
       " 'Banana Republic',\n",
       " 'Old Navy',\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " 'Target',\n",
       " 'Nordstrom',\n",
       " 'Banana Republic',\n",
       " 'Gap',\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " 'Gap',\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " 'Gap',\n",
       " 'Target',\n",
       " 'Home Depot',\n",
       " 'Banana Republic',\n",
       " 'Express',\n",
       " \"Kohl's\",\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " 'Kate Spade',\n",
       " \"Kohl's\",\n",
       " 'Target',\n",
       " 'Banana Republic',\n",
       " '6pm.com',\n",
       " 'Target',\n",
       " 'Nordstrom']"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_brands # so many duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter # Let's count them up!\n",
    "# The final scores\n",
    "scores = [(brand, score) for brand, score in Counter(most_similar_brands).iteritems() if brand not in new_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Express', 1),\n",
       " ('Shoebuy', 1),\n",
       " ('Home Depot', 1),\n",
       " ('Gap', 3),\n",
       " ('Old Navy', 1),\n",
       " ('6pm.com', 1),\n",
       " ('Kate Spade', 1),\n",
       " (\"Kohl's\", 2),\n",
       " ('Nordstrom', 2)]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores # a list of brands and a percentage score of how much I should recommend these brands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's normalize them so that 1s and 2s have meaning by dividing each number by the \n",
    "# number of people we looked at, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Express 10%\n",
      "Shoebuy 10%\n",
      "Home Depot 10%\n",
      "Gap 30%\n",
      "Old Navy 10%\n",
      "6pm.com 10%\n",
      "Kate Spade 10%\n",
      "Kohl's 20%\n",
      "Nordstrom 20%\n"
     ]
    }
   ],
   "source": [
    "for brand, score in scores:\n",
    "    print brand, str(int((score / float(num_similar_users)) * 100))+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GO TO GAP OR KOHL'S OR NORDSTROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_user = ['Target', 'Banana Republic', 'H&M'] # Do you agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collaborative Based Recommendations rely on previous user data, can you see any potential faults here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cold Start Problem\n",
    "    # When we need initial data to even begin recommending\n",
    "    \n",
    "# We rely on what other people think and have done\n",
    "    # whereas content based relies on the actual characteristics of the brandslopkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [sfdat26-env]",
   "language": "python",
   "name": "Python [sfdat26-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
